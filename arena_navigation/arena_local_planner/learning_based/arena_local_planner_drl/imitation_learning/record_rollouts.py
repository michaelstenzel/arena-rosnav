import argparse
from datetime import datetime
import os
from rl_agent.envs.flatland_gym_env import FlatlandEnv
import rospkg
import numpy as np
from pathlib import Path


""" record_rollouts.py
Record rollouts generated by MPC. A rollout/episode/trajectory is a series of (observation, action) pairs recorded over the course of the robots journey
from its starting position to the goal.
The agent will be placed in randomly generated scenarios in a given map.
The synchronization of observation and action pairs occurs entirely in the ObservationCollector (see 
    get_observations_and_action())

Each rollout will be saved to /output/mpc/{map_name}_stage_{stage}/rollout_{datetime}.npz

Args:
    map_name (str): the name of the map used during recording. It will be used to create the path each episode is saved to.
    stage (int): the stage in the training curriculum to be used for recording. This stage is held fixed for each run of this script.
        The training curriculum to be used must be set inline below where FlatlandEnv gets instantiated.
        It will also be used to create the path the episode is saved to. default=1
"""

def save_episode(observations, actions, map_name):
    # save observations and actions in an npz file:
    directory = f'./output/mpc/{args.map_name}_stage_{args.stage}'  # directory to store the rollouts in
    Path(directory).mkdir(parents=True, exist_ok=True)  # make directory if it doesn't exist yet
    date_str = datetime.now().strftime('%Y%m%d_%H-%M-%S')
    path_to_file = f'{directory}/rollout_{date_str}'

    print(f'Saving episode to {path_to_file}')
    np.savez_compressed(
        path_to_file,
        observations = np.array(observations),
        actions = np.array(actions)
    )
    print('Done saving episode.')

ns = ''

parser = argparse.ArgumentParser(description='.')
parser.add_argument('-m', '--map_name', type=str, help='name of the map being recorded on', default="map_small")
parser.add_argument('-stage', '--stage', type=int, metavar="[current stage]", default=1, help='stage to start the simulation with')
args = parser.parse_args()

models_folder_path = rospkg.RosPack().get_path('simulator_setup')
arena_local_planner_drl_folder_path = rospkg.RosPack().get_path(
    'arena_local_planner_drl')

# N.B. move_base_simple tells FlatlandEnv that it should subscribe to /move_base_simple/goal to get the current subgoal
env = FlatlandEnv(ns=ns, PATHS={'robot_setting': os.path.join(models_folder_path, 'robot', 'myrobot.model.yaml'), 'robot_as': os.path.join(arena_local_planner_drl_folder_path,
                               'configs', 'default_settings.yaml'), "model": os.path.join(arena_local_planner_drl_folder_path, "agents", "rule_00"),
                               "curriculum": os.path.join(arena_local_planner_drl_folder_path, "configs", "training_curriculum.yaml")},
                               reward_fnc="rule_00", is_action_space_discrete=False, debug=False, train_mode=True, max_steps_per_episode=600,
                               safe_dist=None, curr_stage=args.stage,
                               move_base_simple=True
                  )  # must set a reward_fnc for the reward calculator, it will return if the episode is done and why

print(f"env: {env}")

obs = env.reset()
episode_observations = []
episode_actions = []
while(True):
    # get current, synchronized, (observation, action) pair
    merged_obs, obs_dict, action = env.observation_collector.get_observations_and_action()

    # compute reward to get reward_info: this tells us whether the episode is done, and if so, why (collision, goal reached, too many steps taken)
    reward, reward_info = env.reward_calculator.get_reward(
            obs_dict['laser_scan'], obs_dict['goal_in_robot_frame'])
    
    # check whether the episode is done
    done, info = env.check_if_done(reward_info)

    if done:
        print(f"done info: {info}")
        if info['done_reason'] == 1:
            # if the episode is done because the robot collided with an obstacle, ignore this episode (clear episode lists)
            print('collision')
            
            episode_observations = []
            episode_actions = []

            # reset obstacles, robot starting position and goal
            env.reset()
        else:
            # robot reached the goal - append last (observation, action) and save the episode
            # (could also be because the maximum number of steps was reached, but this is not currently enforced.
            # To do this, the number of steps must explicitly incremeted after each iteration - uncomment the final line of the while loop)
            if action is not None:
                episode_observations.append(merged_obs)
                episode_actions.append(action)
            save_episode(episode_observations, episode_actions, args.map_name)
            episode_observations = []
            episode_actions = []

            # reset obstacles, robot starting position and goal
            env.reset()
    else:
        # if the episode is not done, save this timesteps's observations and actions to the arrays and continue the episode
        if action is not None:
            episode_observations.append(merged_obs)
            episode_actions.append(action)
    #env._steps_curr_episode += 1  # increase step count to enforce maximum number of steps per episode
