import argparse
from datetime import datetime
import os
from rl_agent.envs.flatland_gym_env import FlatlandEnv
import rospkg
import numpy as np
import subprocess


#TODO docstring
""" record_rollouts.py
Record rollouts generated by MPC.
"""

def save_episode(observations, actions, map_name):
    # save observations and actions in an npz file:
    date_str = datetime.now().strftime('%Y%m%d_%H-%M-%S')
    path = str(f'./output/mpc/{map_name}_stage_{args.stage}/rollout_{date_str}')
    print(f'Saving episode to {path}')
    np.savez_compressed(
        path,
        observations = np.array(observations),
        actions = np.array(actions)
    )
    print('Done saving episode.')

ns = ''

parser = argparse.ArgumentParser(description='.')
parser.add_argument('-m', '--map_name', type=str, help='name of the map being recorded on', default="map_small")
parser.add_argument('-stage', '--stage', type=int, metavar="[current stage]", default=1, help='stage to start the simulation with')
args = parser.parse_args()

models_folder_path = rospkg.RosPack().get_path('simulator_setup')
arena_local_planner_drl_folder_path = rospkg.RosPack().get_path(
    'arena_local_planner_drl')

# N.B. move_base_simple tells FlatlandEnv that it should subscribe to /move_base_simple/goal to get the current subgoal
env = FlatlandEnv(ns=ns, PATHS={'robot_setting': os.path.join(models_folder_path, 'robot', 'myrobot.model.yaml'), 'robot_as': os.path.join(arena_local_planner_drl_folder_path,
                               'configs', 'default_settings.yaml'), "model": os.path.join(arena_local_planner_drl_folder_path, "agents", "rule_00"),
                               "curriculum": os.path.join(arena_local_planner_drl_folder_path, "configs", "training_curriculum.yaml")},
                               reward_fnc="rule_00", is_action_space_discrete=False, debug=False, train_mode=True, max_steps_per_episode=600,
                               safe_dist=None, curr_stage=args.stage,
                               move_base_simple=True
                  )  # must set a reward_fnc for the reward calculator, it will return if the episode is done and why

print(f"env: {env}")

obs = env.reset()
episode_observations = []
episode_actions = []
while(True):
    # get current, synchronized, (observation, action) pair
    merged_obs, obs_dict, action = env.observation_collector.get_observations_and_action()

    # compute reward to get reward_info: this tells us whether the episode is done, and if so, why (collision, goal reached, too many steps taken)
    reward, reward_info = env.reward_calculator.get_reward(
            obs_dict['laser_scan'], obs_dict['goal_in_robot_frame'])
    
    # check whether the episode is done
    done, info = env.check_if_done(reward_info)

    if done:
        print(f"done info: {info}")
        if info['done_reason'] == 1:
            # if the episode is done because the robot collided with an obstacle, ignore this episode (clear episode lists)
            print('collision')
            
            episode_observations = []
            episode_actions = []

            # reset obstacles, robot starting position and goal
            env.reset()
        else:
            # robot reached the goal - append last (observation, action) and save the episode
            # (could also be because the maximum number of steps was reached, but this is not currently enforced.
            # To do this, the number of steps must explicitly incremeted after each iteration - uncomment the final line of the while loop)
            if action is not None:
                episode_observations.append(merged_obs)
                episode_actions.append(action)
            save_episode(episode_observations, episode_actions, args.map_name)
            episode_observations = []
            episode_actions = []

            # reset obstacles, robot starting position and goal
            env.reset()
    else:
        # if the episode is not done, save this timesteps's observations and actions to the arrays and continue the episode
        if action is not None:
            episode_observations.append(merged_obs)
            episode_actions.append(action)
    #env._steps_curr_episode += 1  # increase step count to enforce maximum number of steps per episode
